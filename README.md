# Turn on your local ai-webapp ! ðŸ¤–

### Minimal specs :
Operating System: Linux, Mac or Windows <br>
Memory (RAM): 8GB <br>
Processor: A relatively modern CPU (5 years, 4 cores) <br>
GPU: Integrated GPU works but runs slow <br>

### Recommended specs :
Operating System: Linux, Mac or Windows <br>
Memory (RAM): 16GB <br>
Processor: A relatively modern CPU (5 years, 8 cores) <br>
GPU: Dedicated GPU 6GB VRAM minimal (CUDA is the best) <br>

## Configuration :

### Linux

1. Install Ollama
   
```
curl -fsSL https://ollama.com/install.sh | sh
```

2. Run Ollama
   
```
ollama serve
```

3. Choose a model and make sure it runs

You can find models at <a href='https://ollama.com/search'>ollama.com/search</a>




